{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./scripts/util/platform.py\n",
    "\n",
    "import os\n",
    "\n",
    "def path(path):\n",
    "    return os.path.abspath(os.path.expanduser(path))\n",
    "\n",
    "def filename(filenames, tags):\n",
    "\n",
    "    if not isinstance(filenames, list) or not isinstance(tags, list):\n",
    "        raise ValueError(\"Both arguements must be instances of the 'list' object\")\n",
    "\n",
    "    filenames = sorted(set(map(path, filenames)))\n",
    "\n",
    "    filepath = []\n",
    "\n",
    "    for filename in filenames:\n",
    "        \n",
    "        filepath.append(os.path.splitext(os.path.basename(filename))[0])\n",
    "\n",
    "    filepath = '_'.join(filepath + tags)\n",
    "\n",
    "    return os.path.join(os.path.curdir, 'out', filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement our so called cruncher. It is an abstraction layer on top of the 'lemmatizer' and the 'stemmer' that the package 'nltk' provides its API is quite similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./scripts/cruncher.py\n",
    "\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "class Cruncher:\n",
    "\n",
    "    def __init__(self, method='lemmatizer'):\n",
    "\n",
    "        self.method = method\n",
    "\n",
    "        if method == 'lemmatizer':\n",
    "            self.underlying = WordNetLemmatizer()\n",
    "            self.crunch = self.underlying.lemmatize\n",
    "        elif method == 'stemmer':\n",
    "            self.underlying = PorterStemmer()\n",
    "            self.crunch = self.underlying.stem\n",
    "        else:\n",
    "            raise ValueError(\"'\" + method + \"' is not supported\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we are going to implement our preprocessor. This module's purpose is to preprocess the given tweets and extract only useful information out of them. To be more specific, the preprocessor's role is to:\n",
    "\n",
    "1. Remove any non ascii characters (for example emojis)\n",
    "2. Remove any leading and trailing whitespace characters\n",
    "3. Convert every character to its lower case counterpart\n",
    "4. Remove any urls\n",
    "5. Remove any tags\n",
    "6. Remove any punctuation\n",
    "7. Tokenize tweet at hand and lemmatize each one of its tokens\n",
    "8. while removing any stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./scripts/preprocessor.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from util import platform\n",
    "from cruncher import Cruncher\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    valid_labels = { 'positive', 'negative', 'neutral', 'unknown' }\n",
    "\n",
    "    urlregex = r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))'''\n",
    "    tagregex = r'''@[^\\s]+'''\n",
    "\n",
    "    def __init__(self, filenames, cruncher, save=True):\n",
    "\n",
    "        if not isinstance(cruncher, Cruncher):\n",
    "            raise ValueError(\"'\" + cruncher + \"' is not an instance of 'Cruncher'\")\n",
    "\n",
    "        if not os.path.isdir('out'):\n",
    "            os.mkdir('out')\n",
    "\n",
    "        self.path = platform.filename(filenames, ['preprocessed'])\n",
    "\n",
    "        self.labels = {}\n",
    "\n",
    "        self.tweets = {}\n",
    "\n",
    "        if os.path.isfile(self.path + '.tsv'):\n",
    "\n",
    "            with open(self.path + '.tsv', mode='r', encoding='ascii') as file:\n",
    "                \n",
    "                tokenized_lines = [word_tokenize(line) for line in file.readlines()]\n",
    "\n",
    "                counts = dict(zip(self.valid_labels, [0] * len(self.valid_labels)))\n",
    "\n",
    "                for line in tokenized_lines:\n",
    "\n",
    "                    id, label, tokens = line[0], line[1], line[2:]\n",
    "\n",
    "                    if tokens:\n",
    "                        self.tweets[id] = tokens\n",
    "                        self.labels[id] = label\n",
    "\n",
    "                        counts[label] += 1\n",
    "\n",
    "                for label, count in counts.items():\n",
    "                    print('<LOG>: Loaded', str(count).rjust(5), (\"'\" + label + \"'\").ljust(max(map(len, self.valid_labels)) + 2), 'tweets from', self.path + '.tsv', file=sys.stderr)\n",
    "\n",
    "                return\n",
    "        \n",
    "\n",
    "        for filename in filenames:\n",
    "\n",
    "            with open(filename, mode='r', encoding='utf8') as file:\n",
    "                print('<LOG>: Processing', (\"'\" + filename + \"'\").ljust(max(map(len, filenames)) + 2), file=sys.stderr)\n",
    "\n",
    "                lines = file.readlines()\n",
    "\n",
    "            ignore = set(stopwords.words('english'))\n",
    "\n",
    "            counts = dict(zip(self.valid_labels, [0] * len(self.valid_labels)))\n",
    "\n",
    "            for line in lines:\n",
    "                # Remove any non ascii characters (for example emojis)\n",
    "                line = line.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "                # Remove any leading and trailing whitespace characters\n",
    "                line = line.strip()\n",
    "\n",
    "                # Convert every character to its lower case counterpart\n",
    "                line = line.lower()\n",
    "\n",
    "                # Remove any urls\n",
    "                line = re.sub(self.urlregex, '', line)\n",
    "\n",
    "                # Remove any tags\n",
    "                line = re.sub(self.tagregex, '', line)\n",
    "\n",
    "                # Remove any punctuation\n",
    "                line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "                # Tokenize tweet at hand and lemmatize each one of its tokens\n",
    "                # while removing any stopwords\n",
    "                tokens = word_tokenize(line)\n",
    "\n",
    "                tokens = [cruncher.crunch(token) for token in tokens if token not in ignore]\n",
    "\n",
    "                if tokens[2] in self.valid_labels:\n",
    "\n",
    "                    id, label, tokens = tokens[0], tokens[2], tokens[3:]\n",
    "\n",
    "                    if tokens:\n",
    "                        self.tweets[id] = tokens\n",
    "                        self.labels[id] = label\n",
    "\n",
    "                        counts[label] += 1\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(\"'\" + label + \"' is not a valid label\")\n",
    "\n",
    "            for label, count in counts.items():\n",
    "\n",
    "                print('<LOG>: Saving', str(count).rjust(5), (\"'\" + label + \"'\").ljust(max(map(len, self.valid_labels)) + 2), 'tweets to', self.path + '.tsv', file=sys.stderr)\n",
    "                \n",
    "            if save:\n",
    "\n",
    "                with open(self.path + '.tsv', 'w', encoding='ascii') as file:\n",
    "                    file.write('\\n'.join([id + '\\t' + self.labels[id] + '\\t' + ' '.join(tweet) for id, tweet in self.tweets.items()]))\n",
    "\n",
    "\n",
    "    def by_label(self, labels):\n",
    "\n",
    "        labels = set(labels)\n",
    "\n",
    "        for label in labels:\n",
    "            if label not in self.valid_labels:\n",
    "                raise ValueError(\"'\" + label + \"' is not a valid label\")\n",
    "\n",
    "        return { label: [(id, self.tweets[id]) for id, label in self.labels.items() if label in labels] }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have different dictionaries in our desposal, which record the sentimental value, otherwise known as 'valence', so we need to utilize them in order to sentimentally categorize tokens, which make up the processed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./scripts/dictionary.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "from util import platform\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Dictioanry:\n",
    "\n",
    "    filename = os.path.join(os.path.curdir, 'out', 'dictionary.pkl')\n",
    "\n",
    "    @staticmethod\n",
    "    def convert(value, range_src, range_dst):\n",
    "        min_src, max_src = range_src\n",
    "        min_dst, max_dst = range_dst\n",
    "\n",
    "        return min_dst + (((value - min_src) * (max_dst - min_dst)) / (max_src - min_src))\n",
    "\n",
    "\n",
    "    def __init__(self, root, duplicate_weight=0.5, save=True):\n",
    "\n",
    "        if os.path.isfile(self.filename):\n",
    "            with open(self.filename, mode='rb') as file:\n",
    "                print('<LOG>: Loading word valences from', self.filename, file=sys.stderr)\n",
    "\n",
    "                self.relpaths, self.valences = pickle.load(file)\n",
    "\n",
    "                for i in range(len(self.relpaths)):\n",
    "                    elements = [values[i] for values in self.valences.values()]\n",
    "\n",
    "                    print('<LOG>:', 'The normalized valences of', os.path.basename(self.relpaths[i]).ljust(max(map(lambda path: len(os.path.basename(path)), self.relpaths))), 'are in the range', '[' + '{0:+.4f}'.format(min(elements)), ',', '{0:+.4f}'.format(max(elements)) + ']', file=sys.stderr)\n",
    "\n",
    "                return\n",
    "\n",
    "        if duplicate_weight < 0.0 or duplicate_weight > 1.0:\n",
    "            raise ValueError(\"'duplicate_weight' must be a value in the range [0.0, 1.0]\")\n",
    "\n",
    "        self.relpaths = []\n",
    "\n",
    "        for directory, _, filenames in os.walk(platform.path(root)):\n",
    "            for filename in filenames:\n",
    "                self.relpaths.append(os.path.join(root, directory, filename))\n",
    "\n",
    "        self.valences = {}\n",
    "\n",
    "        for index, fullpath in enumerate(self.relpaths):\n",
    "\n",
    "            valences = {}\n",
    "\n",
    "            with open(fullpath, mode='r', encoding='ascii', errors='ignore') as file:\n",
    "                for line in file.readlines():\n",
    "\n",
    "                    line = line.strip().split()\n",
    "\n",
    "                    words, valence = line[:-1], float(line[-1])\n",
    "\n",
    "                    for word in words:\n",
    "                        if word not in valences:\n",
    "                            valences[word] = valence\n",
    "                        else:\n",
    "                            valences[word] = duplicate_weight * valences[word] + (1.0 - duplicate_weight) * valence\n",
    "\n",
    "            for word, valence in valences.items():\n",
    "                if word not in self.valences:\n",
    "                    self.valences[word] = [0.0] * len(self.relpaths)\n",
    "\n",
    "                self.valences[word][index] = valence\n",
    "\n",
    "            valence_min = np.min(list(self.valences.values()))\n",
    "            valence_max = np.max(list(self.valences.values()))\n",
    "\n",
    "            print('<LOG>:', 'The valences of', os.path.basename(fullpath).ljust(max(map(lambda path: len(os.path.basename(path)), self.relpaths))), 'are in the range', '[' + '{0:+.4f}'.format(valence_min), ',', '{0:+.4f}'.format(valence_max) + ']', file=sys.stderr)\n",
    "\n",
    "            for word in self.valences.keys():\n",
    "                for index, value in enumerate(list(self.valences[word])):\n",
    "                    self.valences[word][index] = self.convert(value, (valence_min, valence_max), (-1, 1))\n",
    "\n",
    "        if save:\n",
    "            if not os.path.isdir('out'):\n",
    "                os.mkdir('out')\n",
    "\n",
    "            with open(self.filename, mode='wb') as file:\n",
    "                pickle.dump((self.relpaths, self.valences), file)\n",
    "\n",
    "                print('<LOG>: Saved word valences to', self.filename, file=sys.stderr)\n",
    "\n",
    "    def per_tweet(self, tweets, vector_range):\n",
    "\n",
    "        valences = [[0.0] * len(self.relpaths)] * len(tweets)\n",
    "\n",
    "        for i, tweet in enumerate(tweets):\n",
    "            for j in range(len(self.relpaths)):\n",
    "                valences[i][j] = np.mean([self.convert(self.valences[token][j], (-1, 1), vector_range) for token in tweet if token in self.valences])\n",
    "\n",
    "        return valences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now present the 'Vectorizer' class, whose purpose is to convert the processed tweets into vectors. We firstly need to supply a method of vectorization among 'bag-of-words', 'tf-idf', 'word-2-vec'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./scripts/vectorizer.py\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from preprocessor import Preprocessor\n",
    "\n",
    "from util import platform\n",
    "\n",
    "class Vectorizer:\n",
    "\n",
    "    vector_size = 300\n",
    "\n",
    "    bowargs = {\n",
    "        \"max_features\": vector_size,\n",
    "        \"stop_words\" : 'english',\n",
    "        \"max_df\" : 0.5,\n",
    "        \"min_df\" : 0.01\n",
    "    }\n",
    "\n",
    "    tfidfargs = {\n",
    "        \"max_df\" : 1.0,\n",
    "        \"min_df\" : 1,\n",
    "        \"max_features\" : vector_size,\n",
    "        \"stop_words\" : 'english'\n",
    "    }\n",
    "\n",
    "    w2vargs = {\n",
    "        \"size\" : vector_size,\n",
    "        \"window\" : 5,\n",
    "        \"min_count\" : 2,\n",
    "        \"sg\" : 1,\n",
    "        \"hs\" : 0,\n",
    "        \"negative\" : 10,\n",
    "        \"workers\" : 2,\n",
    "        \"seed\" : 34\n",
    "    }\n",
    "\n",
    "    supported_methods = { 'word2vec', 'bagofwords', 'tfidf' }\n",
    "\n",
    "\n",
    "    def __init__(self, method='word2vec'):\n",
    "\n",
    "        self.method = re.sub(r'''_|-|\\ ''', '', method)\n",
    "\n",
    "        if self.method == 'word2vec':\n",
    "            self.underlying = Word2Vec(**self.w2vargs)\n",
    "        elif self.method == 'bagofwords':\n",
    "            self.underlying = CountVectorizer(**self.bowargs)\n",
    "        elif self.method == 'tfidf':\n",
    "            self.underlying = TfidfVectorizer(**self.tfidfargs)\n",
    "        else:\n",
    "            raise ValueError(\"'\" + self.method + \"' is not supported\")\n",
    "\n",
    "\n",
    "    def vectorize(self, preprocessor, dictionary, save=True):\n",
    "\n",
    "        if isinstance(preprocessor, list):\n",
    "\n",
    "            path = platform.filename(preprocessor, ['preprocessed', self.method] + (['augmented'] if dictionary else [])) + '.pkl'\n",
    "\n",
    "            if not os.path.isfile(path):\n",
    "                raise ValueError(\"'\" + path + \"' is not a file\")\n",
    "\n",
    "            with open(path, 'rb') as file:\n",
    "                labels, vectors = pickle.load(file)\n",
    "\n",
    "                print('<LOG>: Loaded', len(vectors), 'vectors from', path, '[' + str(len(list(vectors.values())[0])), 'features each]', file=sys.stderr)\n",
    "\n",
    "                return dict(zip(vectors.keys(), labels)), vectors\n",
    "\n",
    "        path = '_'.join([preprocessor.path, self.method] + (['augmented'] if dictionary else [])) + '.pkl'\n",
    "\n",
    "        if not isinstance(preprocessor, Preprocessor):\n",
    "            raise ValueError(\"'preprocessor' is not an instance of 'Preprocessor'\")\n",
    "\n",
    "        return self.process(preprocessor, dictionary, path if save else None)\n",
    "\n",
    "\n",
    "    def process(self, preprocessor, dictionary, path):\n",
    "\n",
    "        tweets = list(preprocessor.tweets.values())\n",
    "\n",
    "        if self.method == 'word2vec':\n",
    "\n",
    "            self.underlying.build_vocab(tweets)\n",
    "\n",
    "            self.underlying.train(sentences=tweets, total_examples=len(tweets), epochs=20)\n",
    "\n",
    "            vectors = [None] * len(tweets)\n",
    "\n",
    "            for i, tweet in enumerate(tweets):\n",
    "                vector = [None] * len(tweet)\n",
    "\n",
    "                for j, token in enumerate(tweet):\n",
    "                    if token in self.underlying.wv:\n",
    "                        vector[j] = self.underlying.wv[token]\n",
    "                    else:\n",
    "                        vector[j] = 2.0 * np.random.randn(self.vector_size) - 1.0\n",
    "\n",
    "                vectors[i] = np.mean(vector, axis=0)\n",
    "\n",
    "        else:\n",
    "\n",
    "            concatenated = [' '.join(tweet) for tweet in tweets]\n",
    "\n",
    "            vectors = self.underlying.fit_transform(concatenated).toarray()\n",
    "\n",
    "        if dictionary:\n",
    "\n",
    "            flattened = list(np.asarray(vectors).flatten())\n",
    "\n",
    "            vmin, vmax = min(flattened), max(flattened)\n",
    "\n",
    "            augmented = [None] * len(vectors)\n",
    "\n",
    "            for i, valences in enumerate(dictionary.per_tweet(tweets, (vmin, vmax))):\n",
    "                augmented[i] = np.concatenate((vectors[i], valences))\n",
    "\n",
    "            vectors = augmented\n",
    "\n",
    "        print('<LOG>: The', ('augmented ' if augmented else '') + 'vectors\\' values are in the range', '[' + '{0:.4f}'.format(vmin), ',', '{0:.4f}'.format(vmax) + ']', file=sys.stderr)\n",
    "\n",
    "        vectors = dict(zip(preprocessor.tweets.keys(), vectors))\n",
    "\n",
    "        if path:\n",
    "            with open(path, 'wb') as file:\n",
    "\n",
    "                pickle.dump((list(preprocessor.labels.values()), vectors), file)\n",
    "\n",
    "                print('<LOG>: Saved', len(vectors), 'vectors to', path, '[' + str(len(list(vectors.values())[0])), 'features each]', file=sys.stderr)\n",
    "\n",
    "        return preprocessor.labels, vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Visualizer' class is responsible for the visualization of our data. The visualization methods currently supported are 'bar_plot', 'word_cloud', 'tsne', 'heat_map'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./scripts/visualizer.py\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from cruncher import Cruncher\n",
    "from preprocessor import Preprocessor\n",
    "from dictionary import Dictioanry\n",
    "from vectorizer import Vectorizer\n",
    "\n",
    "np.random.seed(19680801)\n",
    "\n",
    "class Visualizer:\n",
    "\n",
    "    supported_methods = { 'word_cloud', 'bar_plot', 'tsne', 'heat_map' }\n",
    "\n",
    "    def __init__(self, preprocessor):\n",
    "\n",
    "        if not isinstance(preprocessor, Preprocessor):\n",
    "            raise ValueError(\"'preprocessor' is not an instance of 'Preprocessor'\")\n",
    "\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "\n",
    "    def visualize(self, labels=Preprocessor.valid_labels, method='word_cloud', dictionary=None, model=None, max_words=300):\n",
    "\n",
    "        tokens = []\n",
    "\n",
    "        for _, tweets in self.preprocessor.by_label(labels).items():\n",
    "            for _, tweet in tweets:\n",
    "                tokens += [token for token in tweet]\n",
    "\n",
    "        if method == 'word_cloud':\n",
    "            self.word_cloud(tokens)\n",
    "        elif method == 'bar_plot':\n",
    "            self.bar_plot(tokens)\n",
    "        elif method == 'tsne':\n",
    "            self.tsne(model, max_words)\n",
    "        elif method == 'heat_map':\n",
    "            self.heat_map(tokens, dictionary)\n",
    "        else:\n",
    "            raise ValueError(\"'\" + method + \"' is not supported\")\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def bar_plot(tokens):\n",
    "\n",
    "        count = Counter(tokens)\n",
    "\n",
    "        dataFrame = pd.DataFrame(data=count.most_common(50), columns=['Word', 'Count'])\n",
    "\n",
    "        dataFrame.plot.bar(x='Word',y='Count',figsize = (20,10))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def word_cloud(tokens):\n",
    "\n",
    "        wordcloud = WordCloud(width = 1200, height = 1200,\n",
    "                background_color ='white',\n",
    "                stopwords = set(STOPWORDS),\n",
    "                min_font_size = 14).generate(' '.join(tokens))\n",
    "\n",
    "        plt.figure(figsize = (8, 8), facecolor = None)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout(pad = 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def tsne(model, max_words):\n",
    "\n",
    "        if not isinstance(model, Word2Vec):\n",
    "            raise ValueError(\"'model' is not an instance of 'Word2Vec'\")\n",
    "\n",
    "        if not isinstance(max_words, int) or max_words <= 0:\n",
    "            raise ValueError(\"'max_words' must have an integer value greater than 0\")\n",
    "\n",
    "        labels = []\n",
    "        tokens = []\n",
    "        counter = 0\n",
    "\n",
    "        for word in model.wv.vocab:\n",
    "            tokens.append(model.wv[word])\n",
    "            labels.append(word)\n",
    "            counter +=1\n",
    "            if counter == max_words:\n",
    "                break\n",
    "\n",
    "        tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=5000, random_state=23,)\n",
    "        new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "\n",
    "        for value in new_values:\n",
    "            x.append(value[0])\n",
    "            y.append(value[1])\n",
    "\n",
    "        plt.figure(figsize=(16, 16))\n",
    "        for i in range(len(x)):\n",
    "            plt.scatter(x[i],y[i])\n",
    "            plt.annotate(labels[i],\n",
    "                xy=(x[i], y[i]),\n",
    "                xytext=(5, 2),\n",
    "                textcoords='offset points',\n",
    "                ha='right',\n",
    "                va='bottom')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def heat_map(tokens, dictioanry):\n",
    "\n",
    "        if not isinstance(dictioanry, Dictioanry):\n",
    "            raise ValueError(\"'dictioanry' is not an instance of 'Dictioanry\")\n",
    "\n",
    "        rgb = lambda valence: (255 + Dictioanry.convert(valence, (-1, 1), (0, 255)) + 255) / 3\n",
    "\n",
    "        x, y, c = [], [], []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in dictioanry.valences:\n",
    "                x.append(np.random.rand())\n",
    "                y.append(np.random.rand())\n",
    "                c.append(rgb(np.mean(dictioanry.valences[token])))\n",
    "\n",
    "        plt.scatter(x, y, c=c, alpha=0.8)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "preprocessor = Preprocessor(['..\\\\..\\\\twitter_data\\\\train2017.tsv', '..\\\\..\\\\twitter_data\\\\test2017.tsv'], Cruncher())\n",
    "\n",
    "dictionary = Dictioanry('..\\\\..\\\\lexica')\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "\n",
    "labels, vectors = vectorizer.vectorize(preprocessor, dictionary)\n",
    "\n",
    "visualizer = Visualizer(preprocessor)\n",
    "\n",
    "for method in Visualizer.supported_methods:\n",
    "\n",
    "    visualizer.visualize(method=method, dictionary=dictionary, model=vectorizer.underlying)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers already provided by the python module 'sklearn' have been merged into one 'Classifier', which offers an abstraction layer over the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./scripts/classifier.py\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "class Classifier:\n",
    "\n",
    "    def __init__(self, vectors, labels, method='svm'):\n",
    "\n",
    "        if method == 'svm':\n",
    "            self.underlying = svm.SVC(kernel='sigmoid', gamma='scale', C=1, probability=True)\n",
    "        elif method == 'knn':\n",
    "            self.underlying = KNeighborsClassifier(n_neighbors=100)\n",
    "        else:\n",
    "            raise ValueError(\"'\" + method + \"' is not supported\")\n",
    "\n",
    "        self.underlying.fit(vectors, labels)\n",
    "\n",
    "    def predict(self, unknown):\n",
    "        return self.underlying.predict(unknown)\n",
    "\n",
    "    def predic_proba(self, unknown):\n",
    "        return self.underlying.predict_proba(unknown)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Round-Robin' classifier has been implemented seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./scripts/roundRobin.py\n",
    "\n",
    "import numpy as np\n",
    "from itertools import combinations \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def roundRobin(labels,labeledVector,unknownVector):\n",
    "\n",
    "    comb = combinations(['positive','negative','neutral'], 2) \n",
    "\n",
    "    totalTrainSet = []\n",
    "    for key in labeledVector.keys():\n",
    "        totalTrainSet.append(labeledVector[key])\n",
    "\n",
    "    totalTestSet = []\n",
    "    for key in unknownVector.keys():\n",
    "        totalTestSet.append(unknownVector[key])\n",
    "    \n",
    "    finalTestSet = []\n",
    "    finalTrainSet = []\n",
    "    for combination in comb:\n",
    "        prediction = RR_knn(combination,labeledVector,labels,totalTrainSet,totalTestSet, subProblem = True)\n",
    "\n",
    "        if len(finalTrainSet) == 0:\n",
    "            finalTrainSet = prediction[0]\n",
    "            finalTestSet = prediction[1]\n",
    "        else:\n",
    "            finalTrainSet = appendPrediction(finalTrainSet,prediction[0])\n",
    "            finalTestSet = appendPrediction(finalTestSet,prediction[1])\n",
    "\n",
    "    finalPrediction = RR_knn(['positive','negative','neutral'],labeledVector,labels,totalTrainSet,totalTestSet, subProblem = False)\n",
    "    \n",
    "    return finalPrediction\n",
    "\n",
    "def RR_knn(combination,labeledVector,labels,totalTrainSet,totalTestSet, subProblem = False):\n",
    "    \n",
    "    trainKeys = []\n",
    "    iris_X = []\n",
    "    iris_Y = []\n",
    "    \n",
    "    for comb in combination:\n",
    "        trainKeys += [key for key in labeledVector.keys() if labels[key] == comb]\n",
    "    \n",
    "    for key in trainKeys:\n",
    "        iris_X.append(labeledVector[key])\n",
    "    \n",
    "    for key in trainKeys:\n",
    "        iris_Y.append(labels[key])\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "    knn.fit(iris_X,iris_Y) \n",
    "\n",
    "    if subProblem == True:\n",
    "        prediction = [knn.predict_proba(totalTrainSet),knn.predict_proba(totalTestSet)]\n",
    "    else:\n",
    "        prediction = knn.predict(totalTestSet)\n",
    "\n",
    "    return prediction   \n",
    "\n",
    "def appendPrediction(set, prediction):\n",
    "            newSet = []\n",
    "            for i in range(len(set)):\n",
    "                newSet.append(np.concatenate([set[i],prediction[i]]))\n",
    "            return newSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the master script, which serves as a manager of everything mentioned this far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./scripts/twitter.py\n",
    "\n",
    "from cruncher import Cruncher\n",
    "from preprocessor import Preprocessor\n",
    "from visualizer import Visualizer\n",
    "from vectorizer import Vectorizer\n",
    "\n",
    "import roundRobin as RR\n",
    "\n",
    "from classifier import Classifier\n",
    "from evaluator import Evaluator\n",
    "from dictionary import Dictioanry\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def visualization(train_filename, cruncher_type='lemmatizer'):\n",
    "\n",
    "    preprocessor = Preprocessor([train_filename], Cruncher(cruncher_type))\n",
    "\n",
    "    return Visualizer(preprocessor)\n",
    "\n",
    "def evaluation(filenames, dictionary_root='..\\\\..\\\\lexica', cruncher_type='lemmatizer', vectorizer_type='word2vec', metrics = ['f1-score', 'accuracy-score']):\n",
    "\n",
    "    if not isinstance(filenames, list):\n",
    "        raise ValueError(\"'\" + filenames + \"' is not an instance of 'list'\")\n",
    "\n",
    "    beg = time.time()\n",
    "\n",
    "\n",
    "    vectorizer = Vectorizer(vectorizer_type)\n",
    "\n",
    "    try:\n",
    "\n",
    "        labels, vectors = vectorizer.vectorize(filenames, dictionary_root)\n",
    "\n",
    "    except:\n",
    "\n",
    "        preprocessor = Preprocessor(filenames, Cruncher(cruncher_type))\n",
    "\n",
    "        dictionary = Dictioanry(dictionary_root) if dictionary_root else None\n",
    "\n",
    "        labels, vectors = vectorizer.vectorize(preprocessor, dictionary)\n",
    "\n",
    "\n",
    "    test_ids,  test_labels,  test_vectors  = [], [], []\n",
    "    train_ids, train_labels, train_vectors = [], [], []\n",
    "\n",
    "    for id, label in labels.items():\n",
    "\n",
    "        if label == 'unknown':\n",
    "            test_ids.append(id)\n",
    "            test_labels.append(label)\n",
    "            test_vectors.append(vectors[id])\n",
    "            \n",
    "        else:\n",
    "            train_ids.append(id)\n",
    "            train_labels.append(label)\n",
    "            train_vectors.append(vectors[id])\n",
    "\n",
    "    evaluator = Evaluator()\n",
    "\n",
    "    for classifing in ['knn']:\n",
    "\n",
    "        classifier = Classifier(train_vectors, train_labels, classifing)\n",
    "\n",
    "        predictions = classifier.predict(test_vectors)\n",
    "\n",
    "        for metric in metrics:\n",
    "\n",
    "            value = evaluator.evaluate(dict(zip(test_ids, predictions)), metric)\n",
    "\n",
    "            print('<LOG>: The performance of', \"'\" + classifing + \"'\", 'according to the', (\"'\" + metric + \"'\").ljust(max(map(len, metrics)) + 2), \"metric is\", '{0:.6f}'.format(value))\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print('\\n\\nElapsed time:', '{0:.6f}'.format(end - beg), 'seconds', file=sys.stderr)\n",
    "\n",
    "\n",
    "for vectorizer_type in Vectorizer.supported_methods:\n",
    "    evaluation(['..\\\\..\\\\twitter_data\\\\train2017.tsv', '..\\\\..\\\\twitter_data\\\\test2017.tsv'], vectorizer_type=vectorizer_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
