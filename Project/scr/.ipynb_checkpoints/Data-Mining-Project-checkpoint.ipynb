{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αρχικά επειδή τα εργαλεία που θα χρησιμοποιήσουμε ειναι case sensitive θα μετατρέψουμε το training set μας σε lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('../twitter_data/train2017.tsv', 'r', encoding=\"utf8\") as fileInput:\n",
    "#with open('test.tsv', 'r', encoding=\"utf8\") as fileInput:\n",
    "    text = ''\n",
    "    for line in fileInput:\n",
    "        line = line.lower()\n",
    "        text += line\n",
    "    #print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aφαίρεση των URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', ' ! ', text)\n",
    "text = text.replace('can\\'t',' cant ')\n",
    "text = text.replace(' u ', ' you ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σπάσιμο κειμένου σε tweet για να επεξεργαστούμε το καθένα ξεχωριστά"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetList = text.splitlines()\n",
    "#print(tweetList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σπάσιμο tweet σε λέξεις"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "tokenedTweetList = []\n",
    "for tweet in tweetList:\n",
    "    tokenedTweetList.append(word_tokenize(tweet))\n",
    "#print(tokenedTweetList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αφαίρεση των tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tweet in tokenedTweetList:\n",
    "    for word in tweet:\n",
    "        if word == '@':\n",
    "            tweet.remove(tweet[tweet.index(word)+1])\n",
    "            tweet.remove(tweet[tweet.index(word)])\n",
    "#print(tokenedTweetList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Καθάρισμα σημείων στήξεως"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "clearedTweetList = []\n",
    "for tweet in tokenedTweetList:\n",
    "    for word in tweet:\n",
    "        tokenedTweetList[tokenedTweetList.index(tweet)][tweet.index(word)] = word.strip(punctuation)\n",
    "#print(tokenedTweetList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Καθάρισμα απο stopwords και κενα tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "cleanedTweetList = []\n",
    "\n",
    "for tweet in tokenedTweetList:\n",
    "    cleanedTweet = []\n",
    "    for word in tweet:\n",
    "        if word not in stopwords.words('english') and word != '':\n",
    "            cleanedTweet.append(word);\n",
    "    cleanedTweetList.append(cleanedTweet)\n",
    "#print(cleanedTweetList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('filteredTweets.tsv', 'w', encoding=\"utf8\")\n",
    "\n",
    "for tweet in cleanedTweetList:\n",
    "    for word in tweet:\n",
    "        file.write(word + ' ')\n",
    "    file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα διαχωρήσουμε τα positive/negative/neutral tweets σε διαφορετικά αρχεία για μα διευκολήνουμε την επεξεργασία τους"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('positiveTweets.tsv', 'w', encoding=\"utf8\")\n",
    "\n",
    "for tweet in cleanedTweetList:\n",
    "    if tweet[2] == 'positive':\n",
    "        for word in tweet[3:]:\n",
    "            file.write(word + ' ')\n",
    "        file.write('\\n')\n",
    "file.close()\n",
    "\n",
    "file = open('negativeTweets.tsv', 'w', encoding=\"utf8\")\n",
    "\n",
    "for tweet in cleanedTweetList:\n",
    "    if tweet[2] == 'negative':\n",
    "        for word in tweet[3:]:\n",
    "            file.write(word + ' ')\n",
    "        file.write('\\n')\n",
    "file.close()\n",
    "\n",
    "file = open('neutralTweets.tsv', 'w', encoding=\"utf8\")\n",
    "\n",
    "for tweet in cleanedTweetList:\n",
    "    if tweet[2] == 'neutral':\n",
    "        for word in tweet[3:]:\n",
    "            file.write(word + ' ')\n",
    "        file.write('\\n')\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatized εκδοσεις των παραπάνων αρχείων που θα χρησιμοποιηθούν για ακριβέστερα αποτελέσματα στα dataframes/WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_output = []\n",
    "for tweet in cleanedTweetList:\n",
    "    lemmatized_output.append([lemmatizer.lemmatize(word) for word in tweet])\n",
    "\n",
    "\n",
    "file = open('lemmatizedPositiveTweets.tsv', 'w', encoding=\"utf8\")\n",
    "\n",
    "for tweet in lemmatized_output:\n",
    "    if tweet[2] == 'positive':\n",
    "        for word in tweet[3:]:\n",
    "            file.write(word + ' ')\n",
    "        file.write('\\n')\n",
    "file.close()\n",
    "\n",
    "file = open('lemmatizedNegativeTweets.tsv', 'w', encoding=\"utf8\")\n",
    "\n",
    "for tweet in lemmatized_output:\n",
    "    if tweet[2] == 'negative':\n",
    "        for word in tweet[3:]:\n",
    "            file.write(word + ' ')\n",
    "        file.write('\\n')\n",
    "file.close()\n",
    "\n",
    "file = open('lemmatizedNeutralTweets.tsv', 'w', encoding=\"utf8\")\n",
    "\n",
    "for tweet in lemmatized_output:\n",
    "    if tweet[2] == 'neutral':\n",
    "        for word in tweet[3:]:\n",
    "            file.write(word + ' ')\n",
    "        file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Οι 50 πιο συχνά χρησιμοποιημένες λέξεις στα θετικά tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f429060ee48>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd \n",
    "from nltk import word_tokenize\n",
    "\n",
    "with open('scripts/out/train2017_negative.tsv', 'r', encoding=\"utf8\") as fileInput:\n",
    "    data = ''\n",
    "    for line in fileInput:\n",
    "        line = line.lower()\n",
    "        data += line\n",
    "    \n",
    "tweetList = data.splitlines()\n",
    "\n",
    "positiveTweetList = []\n",
    "for tweet in tweetList:\n",
    "    positiveTweetList.append(word_tokenize(tweet))\n",
    "\n",
    "count = Counter(' ')\n",
    "for tweet in positiveTweetList:\n",
    "    count +=  Counter(tweet)\n",
    "    \n",
    "dataFrame = pd.DataFrame(data =count.most_common(30), columns=['Word', 'Count'])\n",
    "dataFrame.plot.bar(x='Word',y='Count',figsize = (9,7), rot = 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για την δημιουργία του Word Cloud θα χρησιμοποιησουμε το παρακάτω πακέτο(https://anaconda.org/conda-forge/wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oι 50 πιο συχνά χρησιμοποιημένες λέξεις για τα αρνητικά tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "with open('lemmatizedNegativeTweets.tsv', 'r', encoding=\"utf8\") as fileInput:\n",
    "    data = ''\n",
    "    for line in fileInput:\n",
    "        line = line.lower()\n",
    "        data += line\n",
    "    \n",
    "tweetList = data.splitlines()\n",
    "\n",
    "negativeTweetList = []\n",
    "for tweet in tweetList:\n",
    "    negativeTweetList.append(word_tokenize(tweet))\n",
    "\n",
    "count = Counter(' ')\n",
    "for tweet in negativeTweetList:\n",
    "    count +=  Counter(tweet)\n",
    "    \n",
    "dataFrame = pd.DataFrame(data =count.most_common(50), columns=['Word', 'Count'])\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Οι 50 πιο συχνά χρησιμοποιημένες λέξεις για τα ουδέτερα tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "with open('lemmatizedNeutralTweets.tsv', 'r', encoding=\"utf8\") as fileInput:\n",
    "    data = ''\n",
    "    for line in fileInput:\n",
    "        line = line.lower()\n",
    "        data += line\n",
    "    \n",
    "tweetList = data.splitlines()\n",
    "\n",
    "neutralTweetList = []\n",
    "for tweet in tweetList:\n",
    "    neutralTweetList.append(word_tokenize(tweet))\n",
    "\n",
    "count = Counter(' ')\n",
    "for tweet in neutralTweetList:\n",
    "    count +=  Counter(tweet)\n",
    "\n",
    "dataFrame = pd.DataFrame(data =count.most_common(50), columns=['Word', 'Count'])\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Positive Tweets Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "from nltk import word_tokenize\n",
    "\n",
    "positiveText = ''\n",
    "for tweet in positiveTweetList:\n",
    "    for word in tweet:\n",
    "        positiveText = positiveText + word + ' '\n",
    "\n",
    "wordcloud = WordCloud(width = 1200, height = 1200, \n",
    "                background_color ='white',\n",
    "                stopwords = set(STOPWORDS),\n",
    "                min_font_size = 14).generate(positiveText)\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Negative Tweets Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativeText = ''\n",
    "for tweet in negativeTweetList:\n",
    "    for word in tweet:\n",
    "        negativeText = negativeText + word + ' '\n",
    "\n",
    "wordcloud = WordCloud(width = 1200, height = 1200, \n",
    "                background_color ='white',\n",
    "                stopwords = set(STOPWORDS),\n",
    "                min_font_size = 14).generate(negativeText)\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Neutral Tweets Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neutralText = ''\n",
    "for tweet in neutralTweetList:\n",
    "    for word in tweet:\n",
    "        neutralText = neutralText + word + ' '\n",
    "\n",
    "wordcloud = WordCloud(width = 1200, height = 1200, \n",
    "                background_color ='white',\n",
    "                stopwords = set(STOPWORDS),\n",
    "                min_font_size = 14).generate(neutralText)\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmed εκδοση των positive/negative/neutral files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmedTweetList = []\n",
    "for tokens in cleanedTweetList:\n",
    "    stemmedTweetList.append([stemmer.stem(token) for token in tokens ])\n",
    "    \n",
    "file = open('stemmedPositiveTweets.tsv', 'w', encoding=\"utf8\")\n",
    "\n",
    "for tweet in stemmedTweetList:\n",
    "    if tweet[2] == 'posit':\n",
    "        for word in tweet[3:]:\n",
    "            file.write(word + ' ')\n",
    "        file.write('\\n')\n",
    "file.close()\n",
    "\n",
    "file = open('stemmedNegativeTweets.tsv', 'w', encoding=\"utf8\")\n",
    "\n",
    "for tweet in stemmedTweetList:\n",
    "    if tweet[2] == 'neg':\n",
    "        for word in tweet[3:]:\n",
    "            file.write(word + ' ')\n",
    "        file.write('\\n')\n",
    "file.close()\n",
    "\n",
    "file = open('stemmedNeutralTweets.tsv', 'w', encoding=\"utf8\")\n",
    "\n",
    "for tweet in stemmedTweetList:\n",
    "    if tweet[2] == 'neutral':\n",
    "        for word in tweet[3:]:\n",
    "            file.write(word + ' ')\n",
    "        file.write('\\n')\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δημιουργία Bag-of-word και αποθήκευση σε csv ή pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "with open('lemmatizedNeutralTweets.tsv', 'r', encoding=\"utf8\") as fileInput:\n",
    "    data = ''\n",
    "    for line in fileInput:\n",
    "        line = line.lower()\n",
    "        data += line\n",
    "    \n",
    "tweetList = data.splitlines()\n",
    "\n",
    "'''\n",
    "max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    "max_df = 25 means \"ignore terms that appear in more than 25 documents\".\n",
    "The default max_df is 1.0, which means \"ignore terms that appear in more than 100% of the documents\". Thus, the default setting does not ignore any terms\n",
    "min_df = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n",
    "min_df = 5 means \"ignore terms that appear in less than 5 documents\"\n",
    "The default min_df is 1, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms.\n",
    "'''\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 200,stop_words='english',max_df=0.5,min_df=0.01)\n",
    "\n",
    "vectorizedTweetList = vectorizer.fit_transform(tweetList)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "#df = pd.DataFrame(data = vectorizedTweetList.toarray(), columns = vectorizer.get_feature_names())\n",
    "#df.to_csv('neutralBagOfWords.csv',index=False)\n",
    "\n",
    "pickle_out = open('neutralBagOfWords.pickle','wb')\n",
    "pickle.dump(vectorizedTweetList.toarray(), pickle_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "with open('lemmatizedNegativeTweets.tsv', 'r', encoding=\"utf8\") as fileInput:\n",
    "    data = ''\n",
    "    for line in fileInput:\n",
    "        line = line.lower()\n",
    "        data += line\n",
    "    \n",
    "tweetList = data.splitlines()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features = 200,stop_words='english',max_df=0.5,min_df=0.01)\n",
    "\n",
    "vectorizedTweetList = vectorizer.fit_transform(tweetList)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "#df = pd.DataFrame(data = vectorizedTweetList.toarray(), columns = vectorizer.get_feature_names())\n",
    "#df.to_csv('negativeBagOfWords.csv',index=False)\n",
    "\n",
    "pickle_out = open('negativeBagOfWords.pickle','wb')\n",
    "pickle.dump(vectorizedTweetList.toarray(), pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "with open('lemmatizedPositiveTweets.tsv', 'r', encoding=\"utf8\") as fileInput:\n",
    "    data = ''\n",
    "    for line in fileInput:\n",
    "        line = line.lower()\n",
    "        data += line\n",
    "    \n",
    "tweetList = data.splitlines()\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 200,stop_words='english',max_df=0.5,min_df=0.01)\n",
    "\n",
    "vectorizedTweetList = vectorizer.fit_transform(tweetList)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "#df = pd.DataFrame(data = vectorizedTweetList.toarray(), columns = vectorizer.get_feature_names())\n",
    "#df.to_csv('positiveBagOfWords.csv',index=False)\n",
    "\n",
    "pickle_out = open('positiveBagOfWords.pickle','wb')\n",
    "pickle.dump(vectorizedTweetList.toarray(), pickle_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Παραδειγμα για το pickle\n",
    "\n",
    "import pickle\n",
    "pickle_out = open(\"p.pickle\",\"wb\")\n",
    "pickle.dump(model_w2v,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_in = open(\"p.pickle\",\"rb\")\n",
    "model_w2v = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TDIF (παρομοια διαδικασια με το bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "with open('lemmatizedNeutralTweets.tsv', 'r', encoding=\"utf8\") as fileInput:\n",
    "    data = ''\n",
    "    for line in fileInput:\n",
    "        line = line.lower()\n",
    "        data += line\n",
    "    \n",
    "tweetList = data.splitlines()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "tfidf = tfidf_vectorizer.fit_transform(tweetList)\n",
    "print( tfidf.toarray())\n",
    "\n",
    "pickle_out = open('neutralTDIF.pickle','wb')\n",
    "pickle.dump(tfidf.toarray(), pickle_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "with open('lemmatizedPositiveTweets.tsv', 'r', encoding=\"utf8\") as fileInput:\n",
    "    data = ''\n",
    "    for line in fileInput:\n",
    "        line = line.lower()\n",
    "        data += line\n",
    "    \n",
    "tweetList = data.splitlines()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "tfidf = tfidf_vectorizer.fit_transform(tweetList)\n",
    "print( tfidf.toarray())\n",
    "\n",
    "pickle_out = open('positiveTDIF.pickle','wb')\n",
    "pickle.dump(tfidf.toarray(), pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "with open('lemmatizedNegativeTweets.tsv', 'r', encoding=\"utf8\") as fileInput:\n",
    "    data = ''\n",
    "    for line in fileInput:\n",
    "        line = line.lower()\n",
    "        data += line\n",
    "    \n",
    "tweetList = data.splitlines()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "tfidf = tfidf_vectorizer.fit_transform(tweetList)\n",
    "print( tfidf.toarray())\n",
    "\n",
    "pickle_out = open('negativeTDIF.pickle','wb')\n",
    "pickle.dump(tfidf.toarray(), pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qwerty/anaconda3/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "import pickle\n",
    "\n",
    "#To gensim πρεπει αν γινει install στο anaconda \n",
    "with open('lemmatizedNeutralTweets.tsv', 'r', encoding=\"utf8\") as fileInput:\n",
    "    data = ''\n",
    "    for line in fileInput:\n",
    "        line = line.lower()\n",
    "        data += line\n",
    "    \n",
    "tweetList = data.splitlines()\n",
    "\n",
    "neutralTweetList = []\n",
    "for tweet in tweetList:\n",
    "    neutralTweetList.append(word_tokenize(tweet))\n",
    "\n",
    "    \n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            neutralTweetList,\n",
    "            size=50, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34) \n",
    "\n",
    "\n",
    "model_w2v.train(neutralTweetList, total_examples= len(tweetList), epochs=20)\n",
    "\n",
    "pickle_out = open(\"neutralEmbeddedWords.pickle\",\"wb\")\n",
    "pickle.dump(model_w2v,pickle_out)\n",
    "pickle_out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "import pickle\n",
    "\n",
    "with open('lemmatizedNegativeTweets.tsv', 'r', encoding=\"utf8\") as fileInput:\n",
    "    data = ''\n",
    "    for line in fileInput:\n",
    "        line = line.lower()\n",
    "        data += line\n",
    "    \n",
    "tweetList = data.splitlines()\n",
    "\n",
    "negativeTweetList = []\n",
    "for tweet in tweetList:\n",
    "    negativeTweetList.append(word_tokenize(tweet))\n",
    "\n",
    "    \n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            negativeTweetList,\n",
    "            size=50, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34) \n",
    "\n",
    "\n",
    "model_w2v.train(negativeTweetList, total_examples= len(tweetList), epochs=20)\n",
    "\n",
    "pickle_out = open(\"negativeEmbeddedWords.pickle\",\"wb\")\n",
    "pickle.dump(model_w2v,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "import pickle\n",
    "\n",
    "with open('lemmatizedPositiveTweets.tsv', 'r', encoding=\"utf8\") as fileInput:\n",
    "    data = ''\n",
    "    for line in fileInput:\n",
    "        line = line.lower()\n",
    "        data += line\n",
    "    \n",
    "tweetList = data.splitlines()\n",
    "\n",
    "positiveTweetList = []\n",
    "for tweet in tweetList:\n",
    "    positiveTweetList.append(word_tokenize(tweet))\n",
    "\n",
    "    \n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            positiveTweetList,\n",
    "            size=50, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34) \n",
    "\n",
    "\n",
    "model_w2v.train(positiveTweetList, total_examples= len(tweetList), epochs=20)\n",
    "\n",
    "pickle_out = open(\"positiveEmbeddedWords.pickle\",\"wb\")\n",
    "pickle.dump(model_w2v,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
