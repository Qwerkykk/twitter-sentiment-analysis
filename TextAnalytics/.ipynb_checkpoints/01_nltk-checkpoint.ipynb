{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Toolkit\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n",
    "\n",
    "http://nltk.org/book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# It is going to take some minutes!!!!\n",
    "\n",
    "#step 1: Run jupiter as administrator\n",
    "#step 2: uncoment the next line the first time you runthis.\n",
    "#nltk.download('all') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = 'The Athens University of Economics and Business (AUEB) was originally founded in \\\n",
    "1920 under the name of Athens School of Commercial Studies. It was renamed in 1926 as the \\\n",
    "Athens School of Economics and Business, \\\n",
    "a name that was retained until 1989 when it assumed its present name, \\\n",
    "the Athens University of Economics and Business.It is the third oldest \\\n",
    "university in Greece and the oldest in the fields of economics and business. \\\n",
    "Up to 1955 the school offered only one degree in the general area of economics and \\\n",
    "commerce. In 1955 it started two separate programs leading to two separate degrees: \\\n",
    "one in economics and the other in business administration. In 1984 the school was \\\n",
    "divided into three departments, namely the Department of Economics, the Department of \\\n",
    "Business Administration and the Department of Statistics and Informatics.In 1989, the \\\n",
    "university expanded to six departments. From 1999 onwards, the university developed \\\n",
    "even further and nowadays it includes eight academic departments, offering eight \\\n",
    "undergraduate degrees, 28 master\\'s degrees and an equivalent number of doctoral programs.'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Athens University of Economics and Business (AUEB) was originally founded in 1920 under the name of Athens School of Commercial Studies.', 'It was renamed in 1926 as the Athens School of Economics and Business, a name that was retained until 1989 when it assumed its present name, the Athens University of Economics and Business.It is the third oldest university in Greece and the oldest in the fields of economics and business.', 'Up to 1955 the school offered only one degree in the general area of economics and commerce.', 'In 1955 it started two separate programs leading to two separate degrees: one in economics and the other in business administration.', 'In 1984 the school was divided into three departments, namely the Department of Economics, the Department of Business Administration and the Department of Statistics and Informatics.In 1989, the university expanded to six departments.', \"From 1999 onwards, the university developed even further and nowadays it includes eight academic departments, offering eight undergraduate degrees, 28 master's degrees and an equivalent number of doctoral programs.\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Sentence tokenization\n",
    "'''\n",
    "#nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Athens', 'University', 'of', 'Economics', 'and', 'Business', '(', 'AUEB', ')', 'was', 'originally', 'founded', 'in', '1920', 'under', 'the', 'name', 'of', 'Athens', 'School', 'of', 'Commercial', 'Studies', '.', 'It', 'was', 'renamed', 'in', '1926', 'as', 'the', 'Athens', 'School', 'of', 'Economics', 'and', 'Business', ',', 'a', 'name', 'that', 'was', 'retained', 'until', '1989', 'when', 'it', 'assumed', 'its', 'present', 'name', ',', 'the', 'Athens', 'University', 'of', 'Economics', 'and', 'Business.It', 'is', 'the', 'third', 'oldest', 'university', 'in', 'Greece', 'and', 'the', 'oldest', 'in', 'the', 'fields', 'of', 'economics', 'and', 'business', '.', 'Up', 'to', '1955', 'the', 'school', 'offered', 'only', 'one', 'degree', 'in', 'the', 'general', 'area', 'of', 'economics', 'and', 'commerce', '.', 'In', '1955', 'it', 'started', 'two', 'separate', 'programs', 'leading', 'to', 'two', 'separate', 'degrees', ':', 'one', 'in', 'economics', 'and', 'the', 'other', 'in', 'business', 'administration', '.', 'In', '1984', 'the', 'school', 'was', 'divided', 'into', 'three', 'departments', ',', 'namely', 'the', 'Department', 'of', 'Economics', ',', 'the', 'Department', 'of', 'Business', 'Administration', 'and', 'the', 'Department', 'of', 'Statistics', 'and', 'Informatics.In', '1989', ',', 'the', 'university', 'expanded', 'to', 'six', 'departments', '.', 'From', '1999', 'onwards', ',', 'the', 'university', 'developed', 'even', 'further', 'and', 'nowadays', 'it', 'includes', 'eight', 'academic', 'departments', ',', 'offering', 'eight', 'undergraduate', 'degrees', ',', '28', 'master', \"'s\", 'degrees', 'and', 'an', 'equivalent', 'number', 'of', 'doctoral', 'programs', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "word tokenization\n",
    "'''\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 15), ('of', 11), ('and', 11), (',', 8), ('in', 7), ('.', 6), ('Athens', 4), ('Economics', 4), ('was', 4), ('Business', 3)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nh'\\ncount = nltk.FreqDist(tokens)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Counting words\n",
    "'''\n",
    "\n",
    "from collections import Counter\n",
    "count = Counter(tokens)\n",
    "print(count.most_common(10))\n",
    "\n",
    "'''\n",
    "h'\n",
    "count = nltk.FreqDist(tokens)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Removing stopwords\n",
    "'''\n",
    "#nltk.download(u'stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "count = Counter(filtered)\n",
    "print(count.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Creating ngrams\n",
    "'''\n",
    "from nltk.util import ngrams\n",
    "bigrams = [ gram for gram in ngrams(tokens, 2) ]\n",
    "trigrams = [ gram for gram in ngrams(tokens, 3) ]\n",
    "print(trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Universal Part-of-Speech Tagset\n",
    "\n",
    "Tag\tMeaning\tEnglish Examples\n",
    "\n",
    "**ADJ**\tadjective\tnew, good, high, special, big, local\n",
    "\n",
    "**ADP**\tadposition\ton, of, at, with, by, into, under\n",
    "\n",
    "**ADV**\tadverb\treally, already, still, early, now\n",
    "\n",
    "**CONJ**\tconjunction\tand, or, but, if, while, although\n",
    "\n",
    "**DET**\tdeterminer, article\tthe, a, some, most, every, no, which\n",
    "\n",
    "**NOUN**\tnoun\tyear, home, costs, time, Africa\n",
    "\n",
    "**NUM**\tnumeral\ttwenty-four, fourth, 1991, 14:24\n",
    "\n",
    "**PRT**\tparticle\tat, on, out, over per, that, up, with\n",
    "\n",
    "**PRON**\tpronoun\the, their, her, its, my, I, us\n",
    "\n",
    "**VERB**\tverb\tis, say, told, given, playing, would\n",
    "\n",
    "**.**\tpunctuation marks\t. , ; !\n",
    "\n",
    "**X**\tother\tersatz, esprit, dunno, gr8, univeristy\n",
    "\n",
    "**CC** coordinating conjunction\n",
    "**CD** cardinal digit\n",
    "**DT** determiner\n",
    "**EX** existential there (like: “there is” … think of it like “there exists”)\n",
    "**FW** foreign word\n",
    "**IN** preposition/subordinating conjunction\n",
    "**JJ** adjective ‘big’\n",
    "**JJR** adjective, comparative ‘bigger’\n",
    "**JJS** adjective, superlative ‘biggest’\n",
    "**LS** list marker 1)\n",
    "**MD** modal could, will\n",
    "**NN** noun, singular ‘desk’\n",
    "**NNS** noun plural ‘desks’\n",
    "**NNP** proper noun, singular ‘Harrison’\n",
    "**NNPS** proper noun, plural ‘Americans’\n",
    "**PDT** predeterminer ‘all the kids’\n",
    "**POS** possessive ending parent’s\n",
    "**PRP** personal pronoun I, he, she\n",
    "**PRP\\$** possessive pronoun my, his, hers\n",
    "**RB** adverb very, silently,\n",
    "**RBR** adverb, comparative better\n",
    "**RBS** adverb, superlative best\n",
    "**RP** particle give up\n",
    "**TO**, to go ‘to’ the store.\n",
    "**UH** interjection, errrrrrrrm\n",
    "**VB** verb, base form take\n",
    "**VBD** verb, past tense took\n",
    "**VBG** verb, gerund/present participle taking\n",
    "**VBN** verb, past participle taken\n",
    "**VBP** verb, sing. present, non-3d take\n",
    "**VBZ** verb, 3rd person sing. present takes\n",
    "**WDT** wh-determiner which\n",
    "**WP** wh-pronoun who, what\n",
    "**WP\\$** possessive wh-pronoun whose\n",
    "**WRB** wh-abverb where, when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "POS tagging\n",
    "'''\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)\n",
    "\n",
    "# nltk.help.upenn_tagset()\n",
    "# nltk.help.upenn_tagset('CC')\n",
    "# nltk.batch_pos_tag([['this', 'is', 'batch', 'tag', 'test'], ['nltk', 'is', 'text', 'analysis', 'tool']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and lemmatization\n",
    "\n",
    "For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n",
    "\n",
    "am, are, is $\\Rightarrow$ be \n",
    "\n",
    "car, cars, car's, cars' $\\Rightarrow$ car\n",
    "\n",
    "The result of this mapping of text will be something like:\n",
    "\n",
    "the boy's cars are different colors $\\Rightarrow$ the boy car be differ color\n",
    "\n",
    "However, the two words differ in their flavor. *Stemming* usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. *Lemmatization* usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma.\n",
    "\n",
    "For instance:\n",
    "\n",
    "The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
    "\n",
    "The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation.\n",
    "\n",
    "The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context, e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Stemming\n",
    "'''\n",
    "\n",
    "from nltk.stem import StemmerI, RegexpStemmer, LancasterStemmer, ISRIStemmer, PorterStemmer, SnowballStemmer, RSLPStemmer\n",
    "\n",
    "#stemmer = WordNetLemmatizer()\n",
    "#stemmer = LancasterStemmer()\n",
    "#stemmer = SnowballStemmer('english')\n",
    "stemmer = PorterStemmer()\n",
    "stems = [  stemmer.stem(token) for token in tokens ]\n",
    "print(stems)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Lemmatization\n",
    "'''\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('are'))\n",
    "print(lemmatizer.lemmatize('is'))\n",
    "print(lemmatizer.lemmatize(\"bats\"))\n",
    "print(lemmatizer.lemmatize(\"feet\"))\n",
    "print(lemmatizer.lemmatize('is', pos='n'))\n",
    "print(lemmatizer.lemmatize('is', pos='v'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sometimes, the same word can have a multiple lemmas based on the meaning / context.\n",
    "print(lemmatizer.lemmatize(\"stripes\", 'v'))  \n",
    "print(lemmatizer.lemmatize(\"stripes\", 'n'))  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sentence to be lemmatized\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)\n",
    "\n",
    "# Lemmatize list of words and join\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice it didn’t do a good job. Because, ‘are’ is not converted to ‘be’ and ‘hanging’ is not converted to ‘hang’ as expected. This can be corrected if we provide the correct ‘part-of-speech’ tag (POS tag) as the second argument to lemmatize()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "# Lemmatize with POS Tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 2. Lemmatize Single Word with the appropriate POS tag\n",
    "word = 'feet'\n",
    "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "# 3. Lemmatize a Sentence with the appropriate POS tag\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
